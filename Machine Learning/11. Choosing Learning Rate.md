# ğŸ“Œ Choosing a Good Learning Rate in Gradient Descent

## ğŸš€ Importance of Learning Rate (Î±)

- The learning rate controls how much the parameters update at each step of gradient descent.
- **Too small (Î± â†“)** â†’ Very slow convergence.
- **Too large (Î± â†‘)** â†’ May never converge, oscillate, or even diverge.

---

## ğŸ“‰ Diagnosing Learning Rate Issues

### 1ï¸âƒ£ **Signs of a Learning Rate Too Large**

- **Cost Function (J) fluctuates**: If the cost increases and decreases irregularly, the step size may be too big, overshooting the minimum.
- **Cost Function consistently increases**:
  - Learning rate may be too high.
  - Possible bug in the implementation (e.g., incorrect update formula).

### 2ï¸âƒ£ **Signs of a Learning Rate Too Small**

- **Slow convergence**: Gradient descent takes too long to reach the minimum.

---

## ğŸ“ˆ Illustration: Overshooting the Minimum

- The cost function $J$ is plotted on the vertical axis.
- A model parameter (e.g., $w_1$) is on the horizontal axis.

| **Learning Rate Behavior** | **Effect on Cost Function (J)**             |
| -------------------------- | ------------------------------------------- |
| **Too Large (Î± â†‘)**        | Cost jumps back and forth, may increase.    |
| **Too Small (Î± â†“)**        | Cost decreases very slowly.                 |
| **Optimal (Î± â­)**         | Cost consistently decreases at a good pace. |

ğŸ›  **Fix**: Reduce Î± if the cost fluctuates wildly.

---

## ğŸ” Debugging Gradient Descent

1. **Use a Very Small Î± as a Debugging Step**

   - If $J$ **does not** decrease even with a very small Î± â†’ There's likely a **bug** in the code.
   - **Correct update formula:**
     $$
     w := w - \alpha \frac{\partial J}{\partial w}
     $$

2. **Key Debugging Rule**:
   - With a **small enough Î±**, $J$ **should** decrease every iteration.

---

## ğŸ¯ Systematic Approach to Choosing Î±

### 1ï¸âƒ£ **Test a Range of Learning Rates**

- Start with:
  - $ \alpha = 0.001 $
  - Increase stepwise: $ \times 3$ factor  
    (e.g., $0.003 \rightarrow 0.01 \rightarrow 0.03 \rightarrow 0.1$, etc.)

### 2ï¸âƒ£ **Plot Cost Function vs. Iterations**

- Run for a few iterations and observe:
  - **If $J$ decreases rapidly and consistently** â†’ Good Î± âœ…
  - **If $J$ fluctuates or increases** â†’ Reduce Î± âŒ

### 3ï¸âƒ£ **Find Upper & Lower Bounds**

- **Find a value that is too small** (slow convergence).
- **Find a value that is too large** (diverging cost).
- **Select Î± slightly smaller than the largest reasonable Î±**.

---

## ğŸ›  Practical Tips

- **Gradient descent should consistently reduce cost.**
- **If cost increases â†’ Debug your code!**
- **Try different values of $a$ and visualize the cost function trend.**
- **Once $a$ is chosen, optimize further using feature scaling and better initialization strategies.**

---

## ğŸ”œ Next Steps: Custom Features for More Powerful Models

- Beyond linear regression, custom features can help fit **curved** patterns.
- This technique enhances model flexibility beyond straight-line fits.
