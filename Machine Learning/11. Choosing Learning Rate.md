# 📌 Choosing a Good Learning Rate in Gradient Descent

## 🚀 Importance of Learning Rate (α)

- The learning rate controls how much the parameters update at each step of gradient descent.
- **Too small (α ↓)** → Very slow convergence.
- **Too large (α ↑)** → May never converge, oscillate, or even diverge.

---

## 📉 Diagnosing Learning Rate Issues

### 1️⃣ **Signs of a Learning Rate Too Large**

- **Cost Function (J) fluctuates**: If the cost increases and decreases irregularly, the step size may be too big, overshooting the minimum.
- **Cost Function consistently increases**:
  - Learning rate may be too high.
  - Possible bug in the implementation (e.g., incorrect update formula).

### 2️⃣ **Signs of a Learning Rate Too Small**

- **Slow convergence**: Gradient descent takes too long to reach the minimum.

---

## 📈 Illustration: Overshooting the Minimum

- The cost function $J$ is plotted on the vertical axis.
- A model parameter (e.g., $w_1$) is on the horizontal axis.

| **Learning Rate Behavior** | **Effect on Cost Function (J)**             |
| -------------------------- | ------------------------------------------- |
| **Too Large (α ↑)**        | Cost jumps back and forth, may increase.    |
| **Too Small (α ↓)**        | Cost decreases very slowly.                 |
| **Optimal (α ⭐)**         | Cost consistently decreases at a good pace. |

🛠 **Fix**: Reduce α if the cost fluctuates wildly.

---

## 🔍 Debugging Gradient Descent

1. **Use a Very Small α as a Debugging Step**

   - If $J$ **does not** decrease even with a very small α → There's likely a **bug** in the code.
   - **Correct update formula:**
     $$
     w := w - \alpha \frac{\partial J}{\partial w}
     $$

2. **Key Debugging Rule**:
   - With a **small enough α**, $J$ **should** decrease every iteration.

---

## 🎯 Systematic Approach to Choosing α

### 1️⃣ **Test a Range of Learning Rates**

- Start with:
  - $ \alpha = 0.001 $
  - Increase stepwise: $ \times 3$ factor  
    (e.g., $0.003 \rightarrow 0.01 \rightarrow 0.03 \rightarrow 0.1$, etc.)

### 2️⃣ **Plot Cost Function vs. Iterations**

- Run for a few iterations and observe:
  - **If $J$ decreases rapidly and consistently** → Good α ✅
  - **If $J$ fluctuates or increases** → Reduce α ❌

### 3️⃣ **Find Upper & Lower Bounds**

- **Find a value that is too small** (slow convergence).
- **Find a value that is too large** (diverging cost).
- **Select α slightly smaller than the largest reasonable α**.

---

## 🛠 Practical Tips

- **Gradient descent should consistently reduce cost.**
- **If cost increases → Debug your code!**
- **Try different values of $a$ and visualize the cost function trend.**
- **Once $a$ is chosen, optimize further using feature scaling and better initialization strategies.**

---

## 🔜 Next Steps: Custom Features for More Powerful Models

- Beyond linear regression, custom features can help fit **curved** patterns.
- This technique enhances model flexibility beyond straight-line fits.
