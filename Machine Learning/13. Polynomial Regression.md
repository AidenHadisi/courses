# ğŸ“ˆ Polynomial Regression: Fitting Non-Linear Functions

## ğŸ“Œ Overview

So far, we've only fit **straight lines** to our data using multiple linear regression.  
Now, let's extend this idea using **polynomial regression**, allowing us to fit **curves** and model non-linear relationships.

---

## ğŸ”¹ What is Polynomial Regression?

**Polynomial regression** is an extension of linear regression where we introduce **higher-order terms** (powers of $x$) as new features.

### ğŸ”¥ Why Use Polynomial Regression?

- Some relationships in data are **non-linear** and can't be well-represented by a straight line.
- By adding **higher-order terms** (e.g., $x^2, x^3$), we can model more complex patterns.

---

## ğŸ  Example: House Price Prediction

### **Dataset**

- Feature: **Size of house** ($x$) in square feet.
- Target: **Price** of the house.

#### **Linear Model (Straight Line)**

A simple linear regression model:

$$
f(x) = w_1 x + b
$$

ğŸš¨ **Problem:**  
A straight line **does not fit** the data well.

---

## ğŸ”„ Fitting a Curve: Polynomial Regression

### **Quadratic Model (Second-Degree Polynomial)**

Instead of just using $x$, introduce **$x^2$** as a new feature:

$$
f(x) = w_1 x + w_2 x^2 + b
$$

âœ… **Better fit** to the data.  
âŒ **Issue:** A quadratic function **curves downward** eventually, which is **not ideal** for modeling house prices.

---

### **Cubic Model (Third-Degree Polynomial)**

Now, introduce **$x^3$** as well:

$$
f(x) = w_1 x + w_2 x^2 + w_3 x^3 + b
$$

âœ… **Even better fit:** The model can now capture trends where prices increase with house size.  
âœ… **Advantage over Quadratic:** No unexpected downturn in price as size increases.

### **Key Concept: Polynomial Features**

- **Original feature:** $x$
- **New features:** $x^2, x^3, ..., x^n$

This technique allows us to model **non-linear relationships** while still using linear regression techniques.

---

## âš ï¸ Importance of Feature Scaling

When introducing polynomial features, **feature scaling** becomes crucial.

| Feature              | Typical Range     |
| -------------------- | ----------------- |
| $x$ (Size)           | 1 â€“ 1,000         |
| $x^2$ (Size Squared) | 1 â€“ 1,000,000     |
| $x^3$ (Size Cubed)   | 1 â€“ 1,000,000,000 |

ğŸš¨ **Issue:** Large differences in scale can cause gradient descent to struggle.  
âœ… **Solution:** Apply **feature scaling** (e.g., standardization or normalization) to bring all features to a comparable range.

---

## ğŸ”„ Alternative Transformations

Polynomial features aren't the only option!  
Other transformations might work better for certain datasets.

### **Example: Square Root Transformation**

Instead of using $x^2$ and $x^3$, try:

$$
f(x) = w_1 x + w_2 \sqrt{x} + b
$$

âœ… The **square root function** grows **slower** than $x$ but never flattens out or turns downward.  
âœ… Might be a **better choice** for modeling housing prices.

---

## ğŸ¯ How to Choose the Right Features?

- **Experiment!** There is no single best set of features.
- Later in the course, you'll learn how to **evaluate models** and decide which features work best.
- **Feature engineering** is an art that combines **intuition**, **experimentation**, and **evaluation**.

---

## ğŸ› ï¸ Hands-On Practice: Polynomial Regression in Code

### **Optional Lab 1: Implementing Polynomial Regression**

- Try out **$x$, $x^2$, $x^3$** and observe how the model changes.
- Experiment with different polynomial degrees.

### **Optional Lab 2: Using Scikit-Learn**

- **Scikit-learn** is a widely-used **open-source machine learning library**.
- You can fit **polynomial regression models in just a few lines of code**.
- While itâ€™s useful, itâ€™s important to understand how regression works **under the hood** before relying on libraries.

---

## ğŸ‰ Wrapping Up

| Concept                   | Explanation                                                                        |
| ------------------------- | ---------------------------------------------------------------------------------- |
| **Polynomial Regression** | Extends linear regression by adding higher-order features.                         |
| **Feature Engineering**   | Choosing the right transformations (e.g., $x^2$, $\sqrt{x}$) improves predictions. |
| **Feature Scaling**       | Necessary when features have vastly different magnitudes.                          |
| **Scikit-Learn**          | Useful tool for implementing regression models efficiently.                        |

ğŸ“Œ **Next Steps:** Explore **classification algorithms** beyond regression! ğŸš€
