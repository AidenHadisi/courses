# Feature Scaling & Gradient Descent Optimization

## Introduction

Feature scaling improves the efficiency of **Gradient Descent**, allowing it to converge faster by transforming feature values into a similar range.

---

## Feature Size & Parameter Size Relationship

### Example: Predicting House Prices

- **Features**:
  - $x_1$ = Size of house (ranges from **300 to 2000 sq ft**)
  - $x_2$ = Number of bedrooms (ranges from **0 to 5**)
- **Target Output**: House price

| House Features   | Example Values |
| ---------------- | -------------- |
| $x_1$ (Size)     | 2000 sq ft     |
| $x_2$ (Bedrooms) | 5              |
| Price ($y$)      | $500,000$      |

### Understanding Parameter Sizes

#### **Case 1: Large $w_1$, Small $w_2$**

- Given: $w_1 = 50$, $w_2 = 0.1$, $b = 50$
- Prediction:

$$
\hat{y} = 50(2000) + 0.1(5) + 50 = 100,000 + 0.5 + 50
$$

- Result: **$100M$** (far from actual $500K$)
- üî¥ **Poor parameter choice**

#### **Case 2: Small $w_1$, Large $w_2$**

- Given: $w_1 = 0.1$, $w_2 = 50$, $b = 50$
- Prediction:

$$
\hat{y} = 0.1(2000) + 50(5) + 50 = 200 + 250 + 50
$$

- Result: **$500K$** (matches actual price üéØ)
- ‚úÖ **Better parameter choice**

### Key Insight:

- **Larger feature ranges ‚Üí Smaller parameter values**
- **Smaller feature ranges ‚Üí Larger parameter values**

---

## Impact on Gradient Descent

### Scatter Plot Analysis

- **$x_1$ (Size in sq ft) ‚Üí Large range**
- **$x_2$ (Bedrooms) ‚Üí Small range**
- Leads to **skewed parameter updates** in gradient descent.

### Cost Function Contour Plot

- Unequal feature scales cause **elongated ellipses**.
- **Gradient Descent inefficiency**:
  - Moves **slowly** along the elongated direction.
  - Bounces back and forth instead of a direct path.

üî¥ **Problem:** Slow convergence due to inconsistent scaling.

---

## Solution: Feature Scaling

Feature scaling **normalizes** values to a comparable range.

### Goal:

- Transform $x_1$ and $x_2$ so they range **from 0 to 1**.

### Effect on Cost Function:

- Contours become **more circular**.
- **Gradient Descent converges faster** (more direct path to the minimum).

---

## Recap

‚úÖ Large feature ranges slow down Gradient Descent.  
‚úÖ **Feature Scaling** makes all features comparable in scale.  
‚úÖ Improves **convergence speed** significantly.

üîú Next: **How to perform feature scaling!**

---

# Feature Scaling: Implementation

## Why Feature Scaling?

Feature scaling transforms features that have very different ranges into comparable scales, improving **Gradient Descent convergence**.

---

## Methods for Feature Scaling

### 1Ô∏è‚É£ **Min-Max Scaling (Normalization)**

- Rescales values to **[0, 1]** range.
- Formula:

$$
x' = \frac{x}{\max(x)}
$$

- Example:
  - **Feature**: $x_1$ (size in sq ft), range **300 - 2000**
  - **Transformation**:

$$
x'_1 = \frac{x_1}{2000}
$$

- **New range**: **[0.15, 1]**
- Similarly, for **$x_2$ (bedrooms, range 0-5)**:

$$
x'_2 = \frac{x_2}{5}
$$

- **New range**: **[0, 1]**

üìä **Effect:** All features now range from **0 to 1**, making gradient updates uniform.

---

### 2Ô∏è‚É£ **Mean Normalization**

- Centers features around **zero**.
- Formula:

$$
x' = \frac{x - \mu}{\max(x) - \min(x)}
$$

- Formula for mean:

$$
\mu = \frac{1}{m} \sum_{i=1}^{m} x_i
$$

- Example:
  - **Feature**: $x_1$ (size in sq ft)
  - **Mean**: $\mu_1 = 600$
  - **Min-Max Difference**: $2000 - 300$
  - **Transformation**:

$$
x'_1 = \frac{x_1 - 600}{2000 - 300}
$$

- **New range**: **[-0.18, 0.82]**
- Similarly, for **$x_2$ (bedrooms)**:

$$
x'_2 = \frac{x_2 - 2.3}{5 - 0}
$$

- **New range**: **[-0.46, 0.54]**

üìä **Effect:** Data is **centered at 0**, balancing gradient updates.

---

### 3Ô∏è‚É£ **Z-Score Normalization (Standardization)**

- Rescales data based on its **mean** and **standard deviation**.
- Formula:

$$
x' = \frac{x - \mu}{\sigma}
$$

- Formula for $\sigma$ (standard deviation)

$$
\sigma = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2}
$$

- Example:
  - **Feature**: $x_1$ (size in sq ft)
  - **Mean**: $\mu_1 = 600$
  - **Standard Deviation**: $\sigma_1 = 450$
  - **Transformation**:

$$
x'_1 = \frac{x_1 - 600}{450}
$$

- **New range**: **[-0.67, 3.1]**
- Similarly, for **$x_2$ (bedrooms)**:

$$
x'_2 = \frac{x_2 - 2.3}{1.4}
$$

- **New range**: **[-1.6, 1.9]**

üìä **Effect:** Data follows a **normal distribution**, making training stable.

---

## üìà When to Scale Features?

| Feature Type             | Range Example     | Should You Scale?          |
| ------------------------ | ----------------- | -------------------------- |
| **Small range**          | $[-1, 1]$         | ‚úÖ Not necessary, but okay |
| **Moderate range**       | $[-3, 3]$         | ‚úÖ Works fine              |
| **Large range**          | $[-100, 100]$     | ‚ö†Ô∏è Strongly recommended    |
| **Very small range**     | $[-0.001, 0.001]$ | ‚ö†Ô∏è Recommended             |
| **High absolute values** | $[98.6, 105]$     | ‚ö†Ô∏è Recommended             |

üîπ **Rule of Thumb**: Aim for feature values to be around **[-1, 1]**, but small deviations are fine.

---

## üèÜ Final Takeaways

‚úÖ **Feature scaling improves convergence speed** for Gradient Descent.  
‚úÖ **Min-Max Scaling**, **Mean Normalization**, and **Z-Score Normalization** are common techniques.  
‚úÖ **Always scale when feature ranges differ significantly** to avoid slow optimization.

üîú **Next Topic: How to check if Gradient Descent is converging!**
