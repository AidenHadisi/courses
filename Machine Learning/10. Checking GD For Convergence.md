# Gradient Descent Convergence: Understanding Learning Curves

## **Overview**

Gradient Descent is an optimization algorithm used to minimize a cost function $J(w, b)$. To ensure it's working correctly, we analyze **learning curves** to check whether the algorithm is converging.

---

## **Gradient Descent Rule**

Gradient descent updates the parameters $w$ and $b$ iteratively using the following update equations:

$$
w := w - \alpha \frac{\partial J}{\partial w}, \quad
b := b - \alpha \frac{\partial J}{\partial b}
$$

where:

- $\alpha$ = learning rate (step size)
- $\frac{\partial J}{\partial w}, \frac{\partial J}{\partial b}$ = gradients of the cost function

---

## 📈 **Learning Curves: A Visual Tool for Convergence**

A **learning curve** is a graph that helps visualize how well gradient descent is minimizing the cost function $J$.

### 🔹 **How to Plot a Learning Curve**

- **X-axis:** Number of iterations of gradient descent
- **Y-axis:** Value of the cost function $J(w, b)$ at each iteration

### ✅ **What a Good Learning Curve Looks Like**

- **Monotonic Decrease**: $J(w, b)$ should decrease after every iteration.
- **Smooth Convergence**: The curve should gradually flatten as it approaches the minimum.

### ❌ **Signs of Problems**

| Learning Curve Pattern               | Possible Issue                          | Explanation                                    |
| ------------------------------------ | --------------------------------------- | ---------------------------------------------- |
| 🔺 Cost function **increases**       | 🚨 Learning rate ($\alpha$) is too high | The algorithm is overshooting the minimum.     |
| 🌀 Cost fluctuates up & down         | 🚨 Learning rate ($\alpha$) is unstable | Need to decrease $\alpha$.                     |
| 📉 Cost decreases very slowly        | ⚠️ Learning rate ($\alpha$) is too low  | Training is inefficient.                       |
| 📊 Curve flattens early but too high | ⚠️ Model is not learning well           | Possible local minimum or poor initialization. |

---

## 🎯 **How to Determine Convergence**

- **Graph Inspection**: If the cost function $J$ stops decreasing significantly, gradient descent has likely converged.
- **Threshold $\epsilon$ (Epsilon Convergence Test)**:
  - Define a small value $\epsilon$ (e.g., $0.001$ or $10^{-3}$).
  - If $J$ decreases by less than $\epsilon$ in one iteration, consider stopping training.

$$
| J_{\text{current}} - J_{\text{previous}} | < \epsilon
$$

⚠ **However, choosing $\epsilon$ is difficult.** It is often better to rely on **visual analysis** of the learning curve.

---

## 🔢 **Number of Iterations Needed**

The number of iterations required for convergence **varies widely** depending on the problem:

- **Small models**: May converge in **30 iterations**.
- **Complex deep learning models**: May require **100,000+ iterations**.
- **Unknown beforehand**: This is why plotting the learning curve is crucial.

---

## 🎯 **Key Takeaways**

- Plot the **learning curve** (Cost $J$ vs. Iterations) to check for convergence.
- Gradient descent should **always** decrease $J$. If not, adjust the learning rate $\alpha$.
- Convergence is when $J$ flattens; use an **epsilon threshold** as an optional stopping criterion.
- Number of iterations varies by problem—**inspect the curve** rather than relying on a fixed count.

---

## 🔜 **Next Step: Choosing the Right Learning Rate**

Understanding convergence helps us choose an **optimal learning rate** $\alpha$, which will be discussed in the next lecture.
