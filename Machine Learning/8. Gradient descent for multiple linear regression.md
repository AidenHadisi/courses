# ğŸ“š Gradient Descent for Multiple Linear Regression with Vectorization

---

## ğŸ“Œ Multiple Linear Regression in Vector Form

### ğŸ”¹ Notation Recap

- **Parameters:** $w_1, w_2, ..., w_n$ and $b$
- Instead of treating $w_1, ..., w_n$ as separate numbers, we define:
  - **Parameter vector:** $\mathbf{w} \in \mathbb{R}^n$
  - **Feature vector:** $\mathbf{x} \in \mathbb{R}^n$
  - **Bias term:** $b \in \mathbb{R}$ (remains a scalar)

### ğŸ”¹ Model Representation

Using vector notation, we express the hypothesis function as:

$$
f_{\mathbf{w}, b} (\mathbf{x}) = \mathbf{w} \cdot \mathbf{x} + b
$$

where:

- $\mathbf{w} \cdot \mathbf{x}$ is the **dot product**.

---

## ğŸ¯ Cost Function

- Previously written as:

$$ J(w_1, ..., w_n, b) $$

- Now rewritten in vector form:

$$ J(\mathbf{w}, b) $$

where **$J$ takes in $\mathbf{w}$ and $b$ and returns a scalar cost value**.

---

## ğŸ”¥ Gradient Descent for Multiple Regression

### ğŸ”¹ General Update Rule

For each parameter $w_j$:

$$ w_j := w_j - \alpha \frac{\partial J}{\partial w_j} $$

For bias term $b$:

$$ b := b - \alpha \frac{\partial J}{\partial b} $$

where:

- $\alpha$ is the **learning rate**.
- $\frac{\partial J}{\partial w_j}$ and $\frac{\partial J}{\partial b}$ are the **gradients**.

### ğŸ”¹ Gradient Computation

- With **one** feature, gradient descent updates were:

$$ w := w - \alpha \sum (f(x) - y) x $$

$$ b := b - \alpha \sum (f(x) - y) $$

- With **multiple** features:
  - **Error term remains**: $(f(\mathbf{x}) - y)$
  - **Feature vector notation**: $\mathbf{x} = [x_1, x_2, ..., x_n]$
  - Update for each parameter:

$$ w_j := w_j - \alpha \sum (f(\mathbf{x}) - y) x_j, \quad \forall j \in [1, n] $$

- Update for bias:
  $$ b := b - \alpha \sum (f(\mathbf{x}) - y) $$

ğŸ’¡ **Vectorized Update Rule:**

$$ \mathbf{w} := \mathbf{w} - \alpha \nabla\_{\mathbf{w}} J $$

$$ b := b - \alpha \frac{\partial J}{\partial b} $$

---

## âš¡ Alternative Approach: Normal Equation

- **Directly solves** for $\mathbf{w}$ and $b$ in one step **without iterations**.
- Uses **linear algebra techniques** (matrix inversion).
- **Equation:**

$$ \mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$

- **Advantages:**
  - No need to tune a learning rate ($\alpha$).
  - Solves for $\mathbf{w}$ and $b$ in one step.
- **Disadvantages:**
  - **Not generalizable** beyond linear regression.
  - **Computationally expensive** for large feature sets (due to matrix inversion).

ğŸš¨ **Most ML libraries use gradient descent instead of the normal equation for scalability.**

---

## ğŸ› ï¸ Next Steps

- In the **optional lab**, you'll:
  - Implement multiple regression in **Python (NumPy)**.
  - Compute **predictions** $f(\mathbf{x})$.
  - Calculate **cost function** and implement **gradient descent**.
  - Use **vectorized computations** for efficiency.

ğŸ’¡ **Tip:** If NumPy functions feel new, review the previous optional lab on **vectorization**.

---

## ğŸš€ Key Takeaways

âœ… **Multiple linear regression** can be expressed in vector form.  
âœ… **Gradient descent** updates weights and bias iteratively.  
âœ… **Vectorization** improves efficiency by eliminating loops.  
âœ… **Normal equation** is an alternative but **not practical** for large datasets.  
âœ… **Most ML libraries use gradient descent** for training linear regression models.

ğŸ”œ **Next up:** Learn tricks to optimize gradient descent performance!
