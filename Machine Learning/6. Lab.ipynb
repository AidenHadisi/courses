{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple variable regression is similar to the single variable regression, but instead of one feature variable ($x$), we have $n$ features ($\\vec{x}$)\n",
    "\n",
    "Similarity, instead of one weight ($w$), we have one weight for each feature ($\\vec{w}$)\n",
    "\n",
    "In this case, our function (the model) is defined as:\n",
    "\n",
    "$$\n",
    "f_{(\\vec{w},b)}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{(\\vec{w},b)}(\\vec{x}) = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_{(\\vec{w},b)}(\\vec{x}) = \\sum_{j=1}^{n}\\left( w_ix_i  \\right) + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: NDArray[np.number], w: NDArray[np.number], b: float) -> float:\n",
    "    \"\"\" Predict the output of a linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        x: A 1D array-like object containing the input values.\n",
    "        w: A 1D array-like object containing the weights.\n",
    "        b: A float containing the bias.\n",
    "        \n",
    "    Returns:\n",
    "        A float containing the predicted output.\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(x[i] * w[i] for i in range(len(x))) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can simplify this and make it faster using vectorization and numpy features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: NDArray[np.number], w: NDArray[np.number], b: float) -> float:\n",
    "    \"\"\"Predict the output of a linear regression model.\n",
    "\n",
    "    Args:\n",
    "        x: A 1D array-like object containing the input values.\n",
    "        w: A 1D array-like object containing the weights.\n",
    "        b: A float containing the bias.\n",
    "\n",
    "    Returns:\n",
    "        A float containing the predicted output.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function:\n",
    "\n",
    "The cost function will be defined as:\n",
    "\n",
    "$$\n",
    "J_{(\\mathbf{w}, b)} = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f_{(\\mathbf{w},b)}(\\mathbf{x}^i) - y^i \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(\n",
    "    X: NDArray[np.float64], y: NDArray[np.float64], w: NDArray[np.float64], b: float\n",
    ") -> float:\n",
    "    \"\"\"Compute the cost function for a linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X: A 2D array-like object containing the input values.\n",
    "        y: A 1D array-like object containing the target values.\n",
    "        w: A 1D array-like object containing the weights.\n",
    "        b: A float containing the bias.\n",
    "        \n",
    "    Returns:\n",
    "        A float containing the cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = X @ w + b  # Matrix-vector multiplication\n",
    "    errors = (predictions - y) ** 2\n",
    "\n",
    "    return float(np.mean(errors)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The gradient descent is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Repeat Until Convergence}\n",
    "\\{ \n",
    "\n",
    "\\begin{align*}\n",
    "w_j = w_j - \\alpha \\frac{\\partial{J}}{\\partial{\\mathbf{w}}} \\\\\n",
    "b = b - \\alpha \\frac{\\partial{J}}{\\partial{b}}\n",
    "\\end{align*}\n",
    "\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gradient_descent(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    initial_w: NDArray[np.float64],\n",
    "    initial_b: float,\n",
    "    alpha: float = 0.01,\n",
    "    iterations: int = 1000,\n",
    "):\n",
    "    \"\"\"Perform gradient descent to find the optimal weights and bias for a linear regression model.\n",
    "\n",
    "    Args:\n",
    "        X: A 2D array-like object containing the input values.\n",
    "        y: A 1D array-like object containing the target values.\n",
    "        initial_w: A 1D array-like object containing the initial weights.\n",
    "        initial_b: A float containing the initial bias.\n",
    "        alpha: A float containing the learning rate.\n",
    "        iterations: An integer containing the number of iterations to perform.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the weights, bias, and cost.\n",
    "    \"\"\"\n",
    "\n",
    "    w, b = initial_w, initial_b\n",
    "    m, _ = X.shape\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "\n",
    "        w -= alpha * (X.T @ errors) / m\n",
    "        b -= alpha * np.mean(errors)\n",
    "\n",
    "    cost = compute_cost(X, y, w, float(b))\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ]\n",
      "Optimal bias: -0.002235407530932534\n",
      "Cost: 686.7034116665213\n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "w, b, cost = perform_gradient_descent(X_train, y_train, np.zeros(4), 0, 5.0e-7, 1000)\n",
    "\n",
    "print(f\"Optimal weights: {w}\")\n",
    "print(f\"Optimal bias: {b}\")\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w) + b:0.2f}, target value: {y_train[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
